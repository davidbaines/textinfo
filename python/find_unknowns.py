# -*- coding: utf-8 -*-
#NLLB_missing_tokens.ipynb

#Automatically generated by Colaboratory.

#Original file is located at
#    https://colab.research.google.com/drive/15ZOO3I4PVxREWDxBQAUOTaX0GIb-Us3O

# Modified to compare two files (or two folders) and find the characters corresponding to <unk> tokens.

from collections import Counter
from csv import DictWriter
from pathlib import Path
import re
from sacremoses import MosesPunctNormalizer
from typing import IO, Iterable, Iterator, List, Optional, Tuple, cast, Sequence
from tqdm import tqdm

mpn = MosesPunctNormalizer()
mpn.substitutions = [(re.compile(r), sub) for r, sub in mpn.substitutions]


def remove_chars(line, chars):
    char_set = set(chars)
    return ''.join([c for c in line if c not in char_set])


def count_unknowns(tokenized_file,detokenized_file,token):

    unknowns = Counter()
    with open(tokenized_file, "r", encoding='utf-8') as tok, open(detokenized_file, "r", encoding='utf-8') as detok:
        tok_lines = tok.readlines()
        detok_lines = detok.readlines()

        for i,  (tok_line, detok_line) in enumerate(zip(tok_lines,detok_lines)):
            tokens_found = re.findall(f'{token}', tok_line)
            
            if tokens_found:
                #Remove unnecessary characters.
                
                merged_tok_line   = remove_chars(mpn.normalize(tok_line), ["\n"," ","‚ñÅ"])
                merged_detok_line = remove_chars(mpn.normalize(detok_line), ["\n"," "])

                unknown_tokens = merged_detok_line
                common_strings = merged_tok_line.split(token)

                for common_string in common_strings:
                    unknown_tokens = unknown_tokens.replace(common_string,"")
                #print(f"Unknown tokens are {unknown_tokens}")
                #print(f"{merged_detok_line}")
                #print(f"{merged_tok_line}")
                #print(f"Common strings are {common_strings}")
                
                unknowns.update(unknown_tokens)
                
    return unknowns

def alt_count_unknowns(tokenized_file,detokenized_file,token):

    unknowns = Counter()
    with open(tokenized_file, "r", encoding='utf-8') as tok, open(detokenized_file, "r", encoding='utf-8') as detok:
        tok_lines = tok.readlines()
        detok_lines = detok.readlines()

        for i,  (tok_line, detok_line) in enumerate(zip(tok_lines,detok_lines)):
            tokens_found = re.findall(f'{token}', tok_line)
            
            if tokens_found:
                
                #Remove known characters from detok line:
                unknown_chars = remove_chars(mpn.normalize(detok_line), mpn.normalize(tok_line))
                if len(unknown_chars) == len(tokens_found):
                    unknowns.update(unknown_chars)
                else :
                    continue
                    print(f"\nIn line {i+1} of {tokenized_file.name}:")
                    print(f"Found {len(tokens_found)} <unk> tokens in the tokenized line. Found {len(unknown_chars)} characters in the untokenized line that were not in the detokenized line.")
                    print(f"Unknown tokens are {unknown_chars}")
                    print(f"{mpn.normalize(detok_line).strip()}")
                    print(f"{mpn.normalize(tok_line).strip()}")
                
    return unknowns


def main():

    token = "<unk>"
    all_unknowns = dict()

    #src_langs = ['eng_Latn']
    #trg_lang='eng_Latn'

    detokenized_path = Path('C:/Gutenberg/MT/scripture')
    root_path        = Path('C:/Gutenberg/MT/experiments/HuggingFace/NLLB_BT-English')
    tokenized_path   = root_path / 'tokenized'
    output_path      = root_path / 'unknown_tokens'
    summary_csv_file = output_path / "unk_summary.csv"
    detail_csv_file  = output_path / "unk_details.csv"

    detokenized_files = [file for file in detokenized_path.glob("*.txt")]

    for detokenized_file in tqdm(detokenized_files):

        tokenized_file_name = 'token__' + detokenized_file.name
        tokenized_file = tokenized_path / tokenized_file_name

        if tokenized_file.is_file():
            #print(f"Finding unknown tokens in {tokenized_file.name}")
            unknowns = count_unknowns(tokenized_file,detokenized_file,token)
            all_unknowns[tokenized_file] = unknowns
        else:
            print(f"{tokenized_file.name} doesn't exist. Skipping")
    
    summary_count = Counter()
    summary_lines = [f"Unknown tokens of the form '{token}' found in files in {tokenized_path} and the corresponding charater from the untokenized file in {detokenized_path}.\n", \
        "Token, Count\n"]

    detail_lines  = [f"Unknown tokens of the form '{token}' found in {tokenized_path} and their corresponding charater from the untokenized file in {detokenized_path}.\n",\
        "Token,Count,Filename\n"]
    
    #print("These are the unknown tokens:\n")
    for file, counts in all_unknowns.items():
        
        # Update the total unknown counts for all files.
        summary_count.update(counts)

        #print(f"{file}, {counts}\n")
        if len(counts) == 0: 
            detail_lines.append(f"No unknown tokens,0,{file.name}\n")

        else:
            for token, count in counts.most_common():
                detail_lines.append(f"{token},{count},{file.name}\n")
                #print(f"This is a detailled line: {detail_lines[-1]}")
            
    with open(detail_csv_file, 'w', encoding='utf-8', newline='\n') as detail_csv:
        detail_csv.writelines(detail_lines)
    
    print(f'Wrote detailed csv file to {detail_csv_file}')
                
    # And write the data for the first file
    #with open(detail_csv_file, 'a', encoding='utf-8', newline='') as csvfile:
    #    writer = csv.DictWriter(csvfile, fieldnames=column_headers)
    #    for char_dict in chars_list:
    #        writer.writerow(char_dict)

    for token,count in summary_count.most_common():
        summary_lines.append(f"{token},{count}\n")
    
    with open(summary_csv_file, 'w', encoding='utf-8', newline='\n') as summary_csv:
        summary_csv.writelines(summary_lines)
 
    print(f'Wrote summary csv file to {summary_csv_file}')

    #print(all_unknowns)
    #print(type(all_unknowns))


if __name__ == "__main__":
    main()