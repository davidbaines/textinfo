# -*- coding: utf-8 -*-
"""NLLB_missing_tokens.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ZOO3I4PVxREWDxBQAUOTaX0GIb-Us3O

#Setup and Downloads

## Connect to Google Drive
"""

from collections import Counter
#from google.colab import drive
from huggingface_hub import notebook_login
from pathlib import Path
import re
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from typing import IO, Iterable, Iterator, List, Optional, Tuple, cast, Sequence

#drive.mount('/content/gdrive')

"""## Install Transformers, SentencePiece

Make sure your version of Transformers is at least 4.16.0 since some of the functionality we use was introduced in that version:
Then you need to install Git-LFS and setup Git if you haven't already. Uncomment the following instructions and adapt with your name and email:"""

#!apt install git-lfs
#!git config --global user.email "david_baines@sil.org"
#!git config --global user.name "davidbaines"

"""##Connect to HuggingFace Hub"""

#notebook_login()
#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

"""#SILNLP / HuggingFace Compatibility"""

def decode_sp(line: str) -> str:
    return line.replace(" ", "").replace("\u2581", " ").lstrip()

def decode_sp_lines(lines: Iterable[str]) -> Iterable[str]:
    return map(decode_sp, lines)

def write_corpus(corpus_path: Path, sentences: Iterable[str], append: bool = False) -> None:
    with corpus_path.open("a" if append else "w", encoding="utf-8", newline="\n") as file:
        for sentence in sentences:
            file.write(sentence + "\n")

def load_corpus(corpus_path: Path) -> Iterator[str]:
    with corpus_path.open("r", encoding="utf-8-sig") as in_file:
        for line in in_file:
            line = line.strip()
            yield line

#Documentation: https://huggingface.co/transformers/v2.11.0/main_classes/tokenizer.html
model_checkpoint='facebook/nllb-200-distilled-600M'
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_auth_token=True)

"""# Find the characters corresponding to <unk> tokens."""

def remove_chars(line, chars):
    char_set = set(chars)
    return ''.join([c for c in line if c not in char_set])

def count_unknows(tokenized_file,detokenized_file):
    
    unknowns = Counter()

    with open(tokenized_file, "r", encoding='utf-8') as tok, open(detokenized_file, "r", encoding='utf-8') as detok:
        
        detok_lines = detok.readlines()
        # Normalize the punctuation in the original scripture file.
        detok_norm_lines = [mpn.normalize(detok_line) for detok_line in detok_lines]

        tok_lines = tok.readlines()
        # Normalize the tokenized file too:
        tok_norm_lines = [mpn.normalize(tok_line) for tok_line in tok_lines]
        
        for tok_line, detok_line in zip(tok_norm_lines,detok_norm_lines):
            tokens_found = re.findall(f'{token}', tok_line)
            if tokens_found:

                #Remove unnecessary characters.
                tok_line   = remove_chars(tok_line, ["\n"," ","‚ñÅ"])
                detok_line = remove_chars(detok_line, ["\n"," "])

                unknown_tokens = detok_line
                common_strings = tok_line.split(token)
                #print(f"Common strings are {common_strings}")

                for common_string in common_strings:
                    unknown_tokens = unknown_tokens.replace(common_string,"")
                #print(f"Unknown tokens are {unknown_tokens}")

                unknowns.update(unknown_tokens)
    return unknowns


unk = "<unk>"
token = unk
all_unknowns = dict()

src_langs = ['eng_Latn']
trg_lang='eng_Latn'

detokenized_path = Path('C:/Gutenberg/MT/scripture')
tokenized_path = Path('C:/Gutenberg/MT/experiments/HuggingFace/NLLB/tokens')

detokenized_files = detokenized_path.glob("*.txt")

for src_lang in src_langs:   
    for detokenized_file in detokenized_files:

        tokenized_file_name = src_lang + '_tok_' + detokenized_file.name
        tokenized_file = tokenized_path / tokenized_file_name

        if tokenized_file.is_file():
            print(f"Finding unknown tokens in {tokenized_file.name}")
            unknowns = count_unknows(tokenized_file,detokenized_file)
            all_unknowns[tokenized_file.name] = unknowns
        else:
            print(f"{tokenized_file.name} not there yet. Skipping")

print(all_unknowns)


print("These are the unknown tokens:\n")
for file, counts in all_unknowns.items():
    print(file.name,"\ntoken count")
    for token,count in counts:
        print(token, "    ", count)